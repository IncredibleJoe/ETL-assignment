import sys
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
import joblib
import logging
import sqlite3

logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)

TRAIN_DATA = 'housing.csv'
MODEL_NAME = 'model.joblib'
RANDOM_STATE=100


def insert_dataframe_to_db(db_name, table_name, df):
    conn = sqlite3.connect(db_name)

    x = df.to_sql(table_name, conn, if_exists='append', index = False)
    
    if x == None:
        logging.error("Dataframe insert failed")               

    conn.commit()
    conn.close()
    return x

def prepare_data(input_data_path):
    df = pd.read_csv(input_data_path)     

    # Model doesnt evaluate agency column, remove it from dataset 
    try:
        df = df.drop(["AGENCY"], axis=1)
    except KeyError:
        logging.warning("Column 'AGENCY' not provided, no need to remove it") 
   
    df = df.dropna() 
    df.columns = df.columns.str.lower()

    # Check if all columns needed for model are provided
    cols_needed_in_csv = ['longitude','lat','median_age','rooms','bedrooms','pop','households','median_income','median_house_value','ocean_proximity']
    for col in cols_needed_in_csv:
        if col not in df.columns:
            logging.critical(f"Column '{col}' not provided, prediction can not be made") 
            exit()
    
    # Rename column names using mapper dict
    mapper = {  'lat' : 'latitude',             
            'median_age': 'housing_median_age',
            'rooms': 'total_rooms',
            'bedrooms': 'total_bedrooms',
            'pop': 'population',        
    }
    df = df.rename(columns=mapper)   

    # Convert categorical variable into dummy/indicator variables.
    df = pd.get_dummies(df, columns=["ocean_proximity"])     

    # Check if all indicator variables for model are provided, if not create column with all values equal 0
    indicator_vars_needed = ['ocean_proximity_<1H OCEAN', 'ocean_proximity_INLAND',
                                'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY',
                                'ocean_proximity_NEAR OCEAN'
                            ]    
    for col in indicator_vars_needed:
        if col not in df: 
            df[col] = 0            
            df[col] = df[col].astype("uint8") 
    
    # Remove rows with "Null" values
    for col in df.columns:        
       df = df[df[col].astype(str).str.contains("Null")==False]     

    # Remove columns not needed for the model 
    columns_to_remove = ['median_house_value', 'ocean_proximity_Null',
       'ocean_proximity_OUT OF REACH']    
    # errors='ignore' - if column is not provided, continue 
    df_features = df.drop(columns_to_remove, axis=1, errors='ignore')  
    '''Change order of columns - sklearn warning: in future
    versions,providing dataframe with different column order problem'''
    df_features = df_features[['longitude', 'latitude', 'housing_median_age', 'total_rooms',
       'total_bedrooms', 'population', 'households', 'median_income',
       'ocean_proximity_<1H OCEAN', 'ocean_proximity_INLAND',
       'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY',
       'ocean_proximity_NEAR OCEAN']]  

    # # Memory usage optimalization
    # for col in df_features:        
    #     if df_features[col].dtype == "float64":
    #         if df_features[col].max() <= 32767 and df[col].min() >= -32768:
    #             df_features[col] = df_features[col].astype("float32")
    #     else:
    #         continue     

    y = df['median_house_value'].values            
       
    X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=0.2, random_state=RANDOM_STATE)  
    
    return (X_train, X_test, y_train, y_test, df_features)

def train(X_train, y_train):
    # what columns are expected by the model
    X_train.columns

    regr = RandomForestRegressor(max_depth=12)
    regr.fit(X_train,y_train)

    return regr

def predict(X, model):
    Y = model.predict(X)
    return Y

def save_model(model, filename):
    with open(filename, 'wb'):
        joblib.dump(model, filename, compress=3)

def load_model(filename):
    model = joblib.load(filename)
    return model

def demonstrate(model, input_data_df, db_name, table_name):
    predictions = predict(input_data_df, model)
    # Add predicted values generated by model to dataframe
    input_data_df['model_predicted_value'] = predictions
    # Insert dataframe to db
    x = insert_dataframe_to_db(db_name, table_name, input_data_df)
    if x != None:
        # Insert was successfull
        # Get last 3 records to compare it with desired output
        logging.info(input_data_df.tail(3)) 
    

if __name__ == '__main__':
    logging.info('Preparing the data...')
    X_train, X_test, y_train, y_test, input_data = prepare_data(TRAIN_DATA)

    # the model was already trained before
    # logging.info('Training the model...')
    # regr = train(TRAIN_DATA)

    # the model was already saved before into file 'model.joblib'
    # logging.info('Exporting the model...')
    # save_model(regr, MODEL_NAME)

    logging.info('Loading the model...')
    model = load_model(MODEL_NAME)

    logging.info('Calculating train dataset predictions...')
    y_pred_train = predict(X_train, model)
    logging.info('Calculating test dataset predictions...')
    y_pred_test = predict(X_test, model)

    # evaluate model
    logging.info('Evaluating the model...')
    train_error = mean_absolute_error(y_train, y_pred_train)
    test_error = mean_absolute_error(y_test, y_pred_test)

    logging.info('First 5 predictions:')
    logging.info(f'\n{X_test.head()}')
    logging.info(y_pred_test[:5])
    logging.info(f'Train error: {train_error}')
    logging.info(f'Test error: {test_error}') 

    demonstrate(model, input_data, 'housing.db', 'housing')
    